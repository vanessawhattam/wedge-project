{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Big Query is a distributed data warehouse built on a serverless architecture . We’ll discuss this framework in class. In this task you’ll upload all Wedge transaction records to Google Big Query. You’ll want to make sure that the column data types are correctly specified and you’ve properly handled the null values. \n",
    "The requirements for this task change depending on the grade you’re going for. \n",
    "Note: this assignment can be done manually or programmatically. Naturally I’d prefer it be done programmatically so that you get more practice, but that’s not required to get full credit.\n",
    "\n",
    "1. Clean the data\n",
    "    a. I need to split on the delimiter\n",
    "    b. check for a header (and add it if it doesn't have one)\n",
    "    c. fix the \\\\N and \\N  and NULL values - keep as NULL\n",
    "    d. Split them into single month dataframes\n",
    "2. Upload to GBQ\n",
    "    a. Upload each one as a separate table in a new dataset in my GBQ project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import csv\n",
    "import io\n",
    "from io import TextIOWrapper\n",
    "from pandas_gbq import to_gbq\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = os.listdir(\"WedgeZipOfZips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON key stuff\n",
    "service_path = \"C:/Users/vanes/OneDrive/Desktop/Work/MSBA/ADA/wedge_project/\"\n",
    "service_file = 'wedge-project-vw-key.json' # change this to your authentication information  \n",
    "\n",
    "# The project ID that I created in GBQ \n",
    "gbq_proj_id = 'wedge-project-vw'  \n",
    "\n",
    "# Leave this alone because it's the whole key to my GBQ proj\n",
    "private_key = service_path + service_file\n",
    "\n",
    "# Lets me actually acess all my junk \n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)\n",
    "\n",
    "# Tell it where the Zips are located\n",
    "zip_dir = 'WedgeZipOfZips'\n",
    "\n",
    "# The pieces needed for the GBQ upload so it goes to the right place\n",
    "project_id = 'wedge-project-vw'\n",
    "dataset_id = 'wedge_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transArchive_201001_201003.csv: has delimiter: ,\n",
      "transArchive_201004_201006.csv: has delimiter: ,\n",
      "transArchive_201007_201009.csv: has delimiter: ,\n",
      "transArchive_201010_201012.csv: has delimiter: ,\n",
      "transArchive_201101_201103.csv: has delimiter: ,\n",
      "transArchive_201104.csv: has delimiter: ,\n",
      "transArchive_201105.csv: has delimiter: ,\n",
      "transArchive_201106.csv: has delimiter: ,\n",
      "transArchive_201107_201109.csv: has delimiter: ,\n",
      "transArchive_201110_201112.csv: has delimiter: ,\n",
      "transArchive_201201_201203.csv: has delimiter: ,\n",
      "transArchive_201201_201203_inactive.csv: has delimiter: ;\n",
      "transArchive_201204_201206.csv: has delimiter: ,\n",
      "transArchive_201204_201206_inactive.csv: has delimiter: ;\n",
      "transArchive_201207_201209.csv: has delimiter: ,\n",
      "transArchive_201207_201209_inactive.csv: has delimiter: ;\n",
      "transArchive_201210_201212.csv: has delimiter: ,\n",
      "transArchive_201210_201212_inactive.csv: has delimiter: ;\n",
      "transArchive_201301_201303.csv: has delimiter: ,\n",
      "transArchive_201301_201303_inactive.csv: has delimiter: ;\n",
      "transArchive_201304_201306.csv: has delimiter: ,\n",
      "transArchive_201304_201306_inactive.csv: has delimiter: ;\n",
      "transArchive_201307_201309.csv: has delimiter: ,\n",
      "transArchive_201307_201309_inactive.csv: has delimiter: ;\n",
      "transArchive_201310_201312.csv: has delimiter: ,\n",
      "transArchive_201310_201312_inactive.csv: has delimiter: ;\n",
      "transArchive_201401_201403.csv: has delimiter: ,\n",
      "transArchive_201401_201403_inactive.csv: has delimiter: ;\n",
      "transArchive_201404_201406.csv: has delimiter: ,\n",
      "transArchive_201404_201406_inactive.csv: has delimiter: ;\n",
      "transArchive_201407_201409.csv: has delimiter: ,\n",
      "transArchive_201407_201409_inactive.csv: has delimiter: ;\n",
      "transArchive_201410_201412.csv: has delimiter: ,\n",
      "transArchive_201410_201412_inactive.csv: has delimiter: ;\n",
      "transArchive_201501_201503.csv: has delimiter: ,\n",
      "transArchive_201504_201506.csv: has delimiter: ,\n",
      "transArchive_201507_201509.csv: has delimiter: ,\n",
      "transArchive_201510.csv: has delimiter: ,\n",
      "transArchive_201511.csv: has delimiter: ,\n",
      "transArchive_201512.csv: has delimiter: ,\n",
      "transArchive_201601.csv: has delimiter: ,\n",
      "transArchive_201602.csv: has delimiter: ,\n",
      "transArchive_201603.csv: has delimiter: ,\n",
      "transArchive_201604.csv: has delimiter: ,\n",
      "transArchive_201605.csv: has delimiter: ,\n",
      "transArchive_201606.csv: has delimiter: ,\n",
      "transArchive_201607.csv: has delimiter: ,\n",
      "transArchive_201608.csv: has delimiter: ,\n",
      "transArchive_201609.csv: has delimiter: ,\n",
      "transArchive_201610.csv: has delimiter: ,\n",
      "transArchive_201611.csv: has delimiter: ,\n",
      "transArchive_201612.csv: has delimiter: ,\n",
      "transArchive_201701.csv: has delimiter: ,\n"
     ]
    }
   ],
   "source": [
    "delimiters = dict()\n",
    "\n",
    "# Start by reading in all the files again.\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(\"WedgeZipOfZips/\" + this_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file = zf.open(file_name, 'r')\n",
    "            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "            # Read the first line to detect the delimiter\n",
    "            first_line = input_file.readline()\n",
    "            dialect = csv.Sniffer().sniff(sample=first_line, delimiters=[\",\", \";\", \"\\t\"])\n",
    "            detected_delimiter = dialect.delimiter\n",
    "\n",
    "            # Check if the detected delimiter is different from \",\"\n",
    "            if detected_delimiter != \",\":\n",
    "                # Change the delimiter to \",\"\n",
    "                delimiters[file_name] = \",\"\n",
    "            else:\n",
    "                delimiters[file_name] = detected_delimiter\n",
    "\n",
    "            # Reset the file back to the beginning for further processing\n",
    "            input_file.seek(0)\n",
    "\n",
    "            # Now, you can process the file using the appropriate delimiter\n",
    "            for line in input_file:\n",
    "                # Process the data rows here\n",
    "                data = line.strip().split(delimiters[file_name])\n",
    "                \n",
    "\n",
    "            print(f\"{file_name}: has delimiter: {detected_delimiter}\")\n",
    "            input_file.close()  # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_zf in zip_files :\n",
    "    with ZipFile(\"WedgeZipOfZips/\" + this_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files :\n",
    "            input_file = zf.open(file_name,'r')\n",
    "            input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "            \n",
    "            this_delimiter = delimiters[file_name]\n",
    "            \n",
    "            for line in input_file :\n",
    "                print(line.strip().split(this_delimiter))\n",
    "              #  break\n",
    "\n",
    "            input_file.close() # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: transArchive_201001_201003.csv, Has Header: True\n",
      "File: transArchive_201004_201006.csv, Has Header: True\n",
      "File: transArchive_201007_201009.csv, Has Header: True\n",
      "File: transArchive_201010_201012.csv, Has Header: True\n",
      "File: transArchive_201101_201103.csv, Has Header: True\n",
      "File: transArchive_201104.csv, Has Header: True\n",
      "File: transArchive_201105.csv, Has Header: True\n",
      "File: transArchive_201106.csv, Has Header: True\n",
      "File: transArchive_201107_201109.csv, Has Header: True\n",
      "File: transArchive_201110_201112.csv, Has Header: True\n",
      "File: transArchive_201201_201203.csv, Has Header: True\n",
      "File: transArchive_201201_201203_inactive.csv, Has Header: True\n",
      "File: transArchive_201204_201206.csv, Has Header: True\n",
      "File: transArchive_201204_201206_inactive.csv, Has Header: True\n",
      "File: transArchive_201207_201209.csv, Has Header: True\n",
      "File: transArchive_201207_201209_inactive.csv, Has Header: True\n",
      "File: transArchive_201210_201212.csv, Has Header: True\n",
      "File: transArchive_201210_201212_inactive.csv, Has Header: True\n",
      "File: transArchive_201301_201303.csv, Has Header: True\n",
      "File: transArchive_201301_201303_inactive.csv, Has Header: True\n",
      "File: transArchive_201304_201306.csv, Has Header: True\n",
      "File: transArchive_201304_201306_inactive.csv, Has Header: True\n",
      "File: transArchive_201307_201309.csv, Has Header: True\n",
      "File: transArchive_201307_201309_inactive.csv, Has Header: True\n",
      "File: transArchive_201310_201312.csv, Has Header: True\n",
      "File: transArchive_201310_201312_inactive.csv, Has Header: True\n",
      "File: transArchive_201401_201403.csv, Has Header: True\n",
      "File: transArchive_201401_201403_inactive.csv, Has Header: True\n",
      "File: transArchive_201404_201406.csv, Has Header: True\n",
      "File: transArchive_201404_201406_inactive.csv, Has Header: True\n",
      "File: transArchive_201407_201409.csv, Has Header: True\n",
      "File: transArchive_201407_201409_inactive.csv, Has Header: True\n",
      "File: transArchive_201410_201412.csv, Has Header: True\n",
      "File: transArchive_201410_201412_inactive.csv, Has Header: True\n",
      "File: transArchive_201501_201503.csv, Has Header: True\n",
      "File: transArchive_201504_201506.csv, Has Header: True\n",
      "File: transArchive_201507_201509.csv, Has Header: True\n",
      "File: transArchive_201510.csv, Has Header: True\n",
      "File: transArchive_201511.csv, Has Header: False\n",
      "File: transArchive_201512.csv, Has Header: False\n",
      "File: transArchive_201601.csv, Has Header: False\n",
      "File: transArchive_201602.csv, Has Header: False\n",
      "File: transArchive_201603.csv, Has Header: False\n",
      "File: transArchive_201604.csv, Has Header: False\n",
      "File: transArchive_201605.csv, Has Header: False\n",
      "File: transArchive_201606.csv, Has Header: False\n",
      "File: transArchive_201607.csv, Has Header: False\n",
      "File: transArchive_201608.csv, Has Header: False\n",
      "File: transArchive_201609.csv, Has Header: False\n",
      "File: transArchive_201610.csv, Has Header: False\n",
      "File: transArchive_201611.csv, Has Header: False\n",
      "File: transArchive_201612.csv, Has Header: False\n",
      "File: transArchive_201701.csv, Has Header: False\n"
     ]
    }
   ],
   "source": [
    "headers = dict()\n",
    "\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(\"WedgeZipOfZips/\" + this_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file = zf.open(file_name, 'r')\n",
    "            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "            this_delimiter = delimiters[file_name]\n",
    "\n",
    "            # Read the first line to check for the header row\n",
    "            first_line = input_file.readline()\n",
    "\n",
    "            # Check if the first line is a header row (you can customize this check)\n",
    "            is_header = any(keyword in first_line for keyword in ['datetime', 'register_no', 'description', 'trans_status', 'quantity'])\n",
    "\n",
    "            headers[file_name] = is_header\n",
    "\n",
    "            print(f\"File: {file_name}, Has Header: {is_header}\")\n",
    "\n",
    "            input_file.close()  # tidy up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File transArchive_201001_201003.csv has been uploaded to BigQuery table wedge-project-vw.wedge_data.transArchive_201001_201003.\n",
      "File transArchive_201004_201006.csv has been uploaded to BigQuery table wedge-project-vw.wedge_data.transArchive_201004_201006.\n",
      "File transArchive_201007_201009.csv has been uploaded to BigQuery table wedge-project-vw.wedge_data.transArchive_201007_201009.\n"
     ]
    }
   ],
   "source": [
    "# Define your Google BigQuery project and dataset information\n",
    "project_id = 'wedge-project-vw'\n",
    "dataset_id = 'wedge_data'\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "#client = bigquery.Client(project=project_id)\n",
    "\n",
    "# List of headers you want to add (modify as needed)\n",
    "new_headers = [\"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\", \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\", \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\", \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\", \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"]\n",
    "\n",
    "# Store the modified files for BigQuery upload\n",
    "modified_files = []\n",
    "\n",
    "# Define the delimiters dictionary\n",
    "delimiters = {}\n",
    "\n",
    "# Start by reading in all the files again.\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(\"WedgeZipOfZips/\" + this_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file = zf.open(file_name, 'r')\n",
    "            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "            # Read the first line to detect the delimiter\n",
    "            first_line = input_file.readline()\n",
    "            dialect = csv.Sniffer().sniff(sample=first_line, delimiters=[\",\", \";\", \"\\t\"])\n",
    "            detected_delimiter = dialect.delimiter\n",
    "\n",
    "            # Check if the detected delimiter is different from \",\"\n",
    "            if detected_delimiter != \",\":\n",
    "                # Change the delimiter to \",\"\n",
    "                delimiters[file_name] = \",\"\n",
    "            else:\n",
    "                delimiters[file_name] = detected_delimiter\n",
    "\n",
    "            # Reset the file back to the beginning for further processing\n",
    "            input_file.seek(0)\n",
    "\n",
    "            # Read the first line to check for the header row\n",
    "            first_line = input_file.readline()\n",
    "            is_header = any(keyword in first_line for keyword in new_headers)\n",
    "\n",
    "            if not is_header:\n",
    "                # If header is missing, add the new headers\n",
    "                modified_file = [\"\",\"\"] + new_headers + [\"\\n\"]\n",
    "                modified_file.append(first_line)\n",
    "            else:\n",
    "                modified_file = []\n",
    "\n",
    "            # Now, you can process the rest of the file, adding or replacing values as needed\n",
    "            this_delimiter = delimiters[file_name]\n",
    "            for line in input_file:\n",
    "                line = line.strip()\n",
    "                if r'\\\\N' in line or r'\\N' in line:\n",
    "                    line = line.replace(r'\\\\N', 'NULL').replace(r'\\N', 'NULL')\n",
    "                modified_file.append(line)\n",
    "                modified_file.append(\"\\n\")\n",
    "\n",
    "            input_file.close()  # tidy up\n",
    "\n",
    "            modified_files.append((file_name, modified_file))\n",
    "\n",
    "# Now you have a list of modified files to upload to BigQuery\n",
    "for file_name, modified_file in modified_files:\n",
    "    # Upload the modified file to Google BigQuery\n",
    "    table_id = f\"{project_id}.{dataset_id}.{file_name.split('.')[0]}\"\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,  # Skip the header row\n",
    "        autodetect=True,  # Auto-detect schema\n",
    "    )\n",
    "    \n",
    "    with io.StringIO(''.join(modified_file)) as modified_file_io:\n",
    "        load_job = client.load_table_from_file(modified_file_io, table_id, job_config=job_config)\n",
    "    \n",
    "    load_job.result()  # Wait for the job to complete\n",
    "\n",
    "    print(f\"File {file_name} has been uploaded to BigQuery table {table_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 Number of columns 65535 is too many for table 'transArchive_201001_201003_ed8d6c91_b462_4571_8c75_698e041c4105_source'. A table must have no more than 10000 columns.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vanes\\OneDrive\\Desktop\\Work\\MSBA\\ADA\\wedge_project\\task1.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanes/OneDrive/Desktop/Work/MSBA/ADA/wedge_project/task1.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39mStringIO(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(modified_file)) \u001b[39mas\u001b[39;00m modified_file_io:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanes/OneDrive/Desktop/Work/MSBA/ADA/wedge_project/task1.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     load_job \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mload_table_from_file(modified_file_io, table_id, job_config\u001b[39m=\u001b[39mjob_config)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/vanes/OneDrive/Desktop/Work/MSBA/ADA/wedge_project/task1.ipynb#X12sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m load_job\u001b[39m.\u001b[39;49mresult()  \u001b[39m# Wait for the job to complete\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/vanes/OneDrive/Desktop/Work/MSBA/ADA/wedge_project/task1.ipynb#X12sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m has been uploaded to BigQuery table \u001b[39m\u001b[39m{\u001b[39;00mtable_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vanes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\base.py:922\u001b[0m, in \u001b[0;36m_AsyncJob.result\u001b[1;34m(self, retry, timeout)\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_begin(retry\u001b[39m=\u001b[39mretry, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    921\u001b[0m kwargs \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m retry \u001b[39mis\u001b[39;00m DEFAULT_RETRY \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mretry\u001b[39m\u001b[39m\"\u001b[39m: retry}\n\u001b[1;32m--> 922\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(_AsyncJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vanes\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\api_core\\future\\polling.py:261\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[1;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking_poll(timeout\u001b[39m=\u001b[39mtimeout, retry\u001b[39m=\u001b[39mretry, polling\u001b[39m=\u001b[39mpolling)\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    259\u001b[0m     \u001b[39m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    263\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 Number of columns 65535 is too many for table 'transArchive_201001_201003_ed8d6c91_b462_4571_8c75_698e041c4105_source'. A table must have no more than 10000 columns."
     ]
    }
   ],
   "source": [
    "# List of headers you want to add (modify as needed)\n",
    "new_headers = [\"datetime\", \"register_no\", \"emp_no\", \"trans_no\", \"upc\", \"description\", \"trans_type\", \"trans_subtype\", \"trans_status\", \"department\", \"quantity\", \"Scale\", \"cost\", \"unitPrice\", \"total\", \"regPrice\", \"altPrice\", \"tax\", \"taxexempt\", \"foodstamp\", \"wicable\", \"discount\", \"memDiscount\", \"discountable\", \"discounttype\", \"voided\", \"percentDiscount\", \"ItemQtty\", \"volDiscType\", \"volume\", \"VolSpecial\", \"mixMatch\", \"matched\", \"memType\", \"staff\", \"numflag\", \"itemstatus\", \"tenderstatus\", \"charflag\", \"varflag\", \"batchHeaderID\", \"local\", \"organic\", \"display\", \"receipt\", \"card_no\", \"store\", \"branch\", \"match_id\", \"trans_id\"]\n",
    "\n",
    "# Store the modified files for BigQuery upload\n",
    "modified_files = []\n",
    "\n",
    "# Define the delimiters dictionary\n",
    "delimiters = {}\n",
    "\n",
    "# Start by reading in all the files again.\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(\"WedgeZipOfZips/\" + this_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file = zf.open(file_name, 'r')\n",
    "            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "            # Read the first line to detect the delimiter\n",
    "            first_line = input_file.readline()\n",
    "            dialect = csv.Sniffer().sniff(sample=first_line, delimiters=[\",\", \";\", \"\\t\"])\n",
    "            detected_delimiter = dialect.delimiter\n",
    "\n",
    "            # Check if the detected delimiter is different from \",\"\n",
    "            if detected_delimiter != \",\":\n",
    "                # Change the delimiter to \",\"\n",
    "                delimiters[file_name] = \",\"\n",
    "            else:\n",
    "                delimiters[file_name] = detected_delimiter\n",
    "\n",
    "            # Reset the file back to the beginning for further processing\n",
    "            input_file.seek(0)\n",
    "\n",
    "            # Read the first line to check for the header row\n",
    "            first_line = input_file.readline()\n",
    "            is_header = any(keyword in first_line for keyword in new_headers)\n",
    "\n",
    "            if not is_header:\n",
    "                # If header is missing, add the new headers\n",
    "                modified_file = [\",\".join(new_headers) + \"\\n\"]\n",
    "            else:\n",
    "                modified_file = [first_line]\n",
    "\n",
    "            # Now, you can process the rest of the file, adding or replacing values as needed\n",
    "            this_delimiter = delimiters[file_name]\n",
    "            for line in input_file:\n",
    "                line = line.strip()\n",
    "                if r'\\\\N' in line or r'\\N' in line:\n",
    "                    line = line.replace(r'\\\\N', 'NULL').replace(r'\\N', 'NULL')\n",
    "                modified_file.append(line)\n",
    "\n",
    "            input_file.close()  # tidy up\n",
    "\n",
    "            modified_files.append((file_name, modified_file))\n",
    "\n",
    "# Now you have a list of modified files to upload to BigQuery\n",
    "for file_name, modified_file in modified_files:\n",
    "    # Upload the modified file to Google BigQuery\n",
    "    table_id = f\"{project_id}.{dataset_id}.{file_name.split('.')[0]}\"\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=0,  # No need to skip the header row\n",
    "        autodetect=True,  # Auto-detect schema\n",
    "    )\n",
    "    \n",
    "    with io.StringIO(''.join(modified_file)) as modified_file_io:\n",
    "        load_job = client.load_table_from_file(modified_file_io, table_id, job_config=job_config)\n",
    "    \n",
    "    load_job.result()  # Wait for the job to complete\n",
    "\n",
    "    print(f\"File {file_name} has been uploaded to BigQuery table {table_id}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
