{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the packages needed\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The files that have semicolons live in the semicolon file\n",
    "zip_folder = 'semicolon_files'\n",
    "\n",
    "# Put them back in a different folder once they're fixed\n",
    "# Not a super clean way to do it, but I've done it now\n",
    "output_folder = 'wedge_zips'\n",
    "\n",
    "# Ensure the output folder exists\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "# Iterate over the zip files in the zip folder\n",
    "for zip_file_name in os.listdir(zip_folder):\n",
    "\n",
    "    #Check if the file ends with .zip so we know to unzip it\n",
    "    if zip_file_name.endswith('.zip'):\n",
    "        zip_file_path = os.path.join(zip_folder, zip_file_name)\n",
    "        \n",
    "        # Create a new zip file for writing the modified CSV\n",
    "        output_zip_file_path = os.path.join(output_folder, zip_file_name)\n",
    "        with ZipFile(output_zip_file_path, 'w') as output_zip:\n",
    "            \n",
    "            # Extract and process the CSV file\n",
    "            with ZipFile(zip_file_path, 'r') as zf:\n",
    "                for file_name in zf.namelist():\n",
    "                    if file_name.endswith('.csv'):\n",
    "\n",
    "                        #keep the file name the same\n",
    "                        csv_file_name = file_name\n",
    "                        with zf.open(csv_file_name, 'r') as input_file:\n",
    "                            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "                            \n",
    "                            # Create a new CSV file within the output zip\n",
    "                            output_file_name = os.path.splitext(csv_file_name)[0] + '.csv'\n",
    "                            # basically just rewrite the whole csv line by line\n",
    "                            output_zip.writestr(output_file_name, '\\n'.join(','.join(line.split(';')) for line in input_file))\n",
    "        \n",
    "print(\"CSV files have been processed and saved back to their respective zip files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    #\"datetime\": \"TIMESTAMP\", #1\n",
    "    \"register_no\": float, #2\n",
    "    \"emp_no\": float, #3\n",
    "    \"trans_no\": float, #4\n",
    "    \"upc\": str, #5\n",
    "    \"description\": str, #6\n",
    "    \"trans_type\": str, #7\n",
    "    \"trans_subtype\": str, #8\n",
    "    \"trans_status\": str, #9\n",
    "    \"department\": float, #10\n",
    "    \"quantity\": float, #11\n",
    "    \"Scale\": float, #12\n",
    "    \"cost\": float, #13\n",
    "    \"unitPrice\": float, #14\n",
    "    \"total\": float, #15\n",
    "    \"regPrice\": float, #16\n",
    "    \"altPrice\": float, #17\n",
    "    \"tax\": float, #18\n",
    "    \"taxexempt\": float, #19\n",
    "    \"foodstamp\": float, #20\n",
    "    \"wicable\": float, #21\n",
    "    \"discount\": float, #22\n",
    "    \"memDiscount\": float, #23\n",
    "    \"discountable\": float, #24\n",
    "    \"discounttype\": float, #25\n",
    "    \"voided\": float, #26\n",
    "    \"percentDiscount\": float, #27\n",
    "    \"ItemQtty\": float, #28\n",
    "    \"volDiscType\": float, #29\n",
    "    \"volume\": float, #30\n",
    "    \"VolSpecial\": float, #31\n",
    "    \"mixMatch\": float, #32\n",
    "    \"matched\": float, #33\n",
    "    \"memType\": bool, #34\n",
    "    \"staff\": bool, #35\n",
    "    \"numflag\": float, #36\n",
    "    \"itemstatus\": float, #37\n",
    "    \"tenderstatus\": float, #38\n",
    "    \"charflag\": str, #39\n",
    "    \"varflag\": float, #40\n",
    "    \"batchHeaderID\": bool, #41\n",
    "    \"local\": float, #42\n",
    "    \"organic\": float, #43\n",
    "    \"display\": bool, #44\n",
    "    \"receipt\": float, #45\n",
    "    \"card_no\": float, #46\n",
    "    \"store\": float, #47\n",
    "    \"branch\": float, #48\n",
    "    \"match_id\": float, #49\n",
    "    \"trans_id\": float, #50\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The folder where our files with no header live\n",
    "# Which we discovered using the task1_testing file\n",
    "zip_folder = \"no_headers\"\n",
    "\n",
    "# List all the zip files in the folder\n",
    "zip_files = [file for file in os.listdir(zip_folder) if file.endswith(\".zip\")]\n",
    "\n",
    "# Create a new directory to store the files with headers\n",
    "# Getting messy again with multiple folders\n",
    "output_folder = \"wedge_zips\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "headers = dict()\n",
    "\n",
    "for this_zf in zip_files:\n",
    "    with ZipFile(\"WedgeZipOfZips/\" + this_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            input_file = zf.open(file_name, 'r')\n",
    "            input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "            # Read the first line to check for the header row\n",
    "            first_line = input_file.readline()\n",
    "\n",
    "            # Check if the first line is a header row (you can customize this check)\n",
    "            is_header = any(keyword in first_line for keyword in column_types.keys())\n",
    "\n",
    "            headers[file_name] = is_header\n",
    "\n",
    "            if not is_header:\n",
    "                # The first line is not a header, so add it\n",
    "                header_row = ','.join(column_types.keys()) + '\\n'\n",
    "                input_file.seek(0)  # Go back to the beginning of the file\n",
    "                file_contents = input_file.read()\n",
    "                input_file.close()\n",
    "\n",
    "                # Create a subfolder with the same name as the original file (without extension) in \"with_headers\"\n",
    "                subfolder_path = os.path.join(output_folder, os.path.splitext(file_name)[0])\n",
    "                os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "                # Write the header and file contents into a CSV file in the subfolder\n",
    "                output_file_path = os.path.join(subfolder_path, file_name)\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(header_row + file_contents)\n",
    "\n",
    "            print(f\"File: {file_name}, Has Header: {is_header}\")\n",
    "\n",
    "# Now, create a zip file for each subfolder in the \"with_headers\" folder\n",
    "for foldername, subfolders, filenames in os.walk(output_folder):\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(output_folder, subfolder)\n",
    "        zip_file_path = os.path.join(output_folder, subfolder + '.zip')\n",
    "        with ZipFile(zip_file_path, 'w') as new_zip:\n",
    "            for root, dirs, files in os.walk(subfolder_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    new_zip.write(file_path, os.path.relpath(file_path, subfolder_path))\n",
    "\n",
    "# Clean up the subfolders in \"with_headers\" after creating the zip files\n",
    "for subfolder in os.listdir(output_folder):\n",
    "    subfolder_path = os.path.join(output_folder, subfolder)\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        for file in os.listdir(subfolder_path):\n",
    "            file_path = os.path.join(subfolder_path, file)\n",
    "            os.remove(file_path)\n",
    "        os.rmdir(subfolder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON key path\n",
    "service_path = \"C:/Users/vanes/OneDrive/Desktop/Work/MSBA/ADA/wedge_project/\"\n",
    "service_file = 'wedge-project-vw-key.json'  \n",
    "\n",
    "# Gotta be credentialed\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "\n",
    "# The pieces needed for the GBQ upload so it goes to the right place\n",
    "project_id = 'wedge-project-vw'\n",
    "dataset_id = 'wedge_data2'\n",
    "\n",
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full schema for the table as a list of dictionaries\n",
    "schema = [\n",
    "    {\"name\": \"datetime\", \"type\": \"TIMESTAMP\"}, #1\n",
    "    {\"name\": \"register_no\", \"type\": \"FLOAT\"}, #2\n",
    "    {\"name\": \"emp_no\", \"type\": \"FLOAT\"}, #3\n",
    "    {\"name\": \"trans_no\", \"type\": \"FLOAT\"}, #4 \n",
    "    {\"name\": \"upc\", \"type\": \"STRING\"}, #5\n",
    "    {\"name\": \"description\", \"type\": \"STRING\"}, #6\n",
    "    {\"name\": \"trans_type\", \"type\": \"STRING\"}, #7 \n",
    "    {\"name\": \"trans_subtype\", \"type\": \"STRING\"}, #8 \n",
    "    {\"name\": \"trans_status\", \"type\": \"STRING\"}, #9 \n",
    "    {\"name\": \"department\", \"type\": \"FLOAT\"}, #10 \n",
    "    {\"name\": \"quantity\", \"type\": \"FLOAT\"}, #11\n",
    "    {\"name\": \"Scale\", \"type\": \"FLOAT\"}, # 12\n",
    "    {\"name\": \"cost\", \"type\": \"FLOAT\"}, # 13\n",
    "    {\"name\": \"unitPrice\", \"type\": \"FLOAT\"}, #14\n",
    "    {\"name\": \"total\", \"type\": \"FLOAT\"}, #15\n",
    "    {\"name\": \"regPrice\", \"type\": \"FLOAT\"}, #16\n",
    "    {\"name\": \"altPrice\", \"type\": \"FLOAT\"}, # 17\n",
    "    {\"name\": \"tax\", \"type\": \"FLOAT\"}, #18\n",
    "    {\"name\": \"taxexempt\", \"type\": \"FLOAT\"}, #19\n",
    "    {\"name\": \"foodstamp\", \"type\": \"FLOAT\"}, #20\n",
    "    {\"name\": \"wicable\", \"type\": \"FLOAT\"}, #21\n",
    "    {\"name\": \"discount\", \"type\": \"FLOAT\"}, #22\n",
    "    {\"name\": \"memDiscount\", \"type\": \"FLOAT\"}, #23\n",
    "    {\"name\": \"discountable\", \"type\": \"FLOAT\"}, #24\n",
    "    {\"name\": \"discounttype\", \"type\": \"FLOAT\"}, #25\n",
    "    {\"name\": \"voided\", \"type\": \"FLOAT\"}, #26\n",
    "    {\"name\": \"percentDiscount\", \"type\": \"FLOAT\"}, #27\n",
    "    {\"name\": \"ItemQtty\", \"type\": \"FLOAT\"}, #28\n",
    "    {\"name\": \"volDiscType\", \"type\": \"FLOAT\"}, #29\n",
    "    {\"name\": \"volume\", \"type\": \"FLOAT\"}, #30\n",
    "    {\"name\": \"VolSpecial\", \"type\": \"FLOAT\"}, #31\n",
    "    {\"name\": \"mixMatch\", \"type\": \"FLOAT\"}, #32\n",
    "    {\"name\": \"matched\", \"type\": \"FLOAT\"}, #33\n",
    "    {\"name\": \"memType\", \"type\": \"BOOLEAN\"}, #34\n",
    "    {\"name\": \"staff\", \"type\": \"BOOLEAN\"}, #35\n",
    "    {\"name\": \"numflag\", \"type\": \"FLOAT\"}, #36\n",
    "    {\"name\": \"itemstatus\", \"type\": \"FLOAT\"}, #37\n",
    "    {\"name\": \"tenderstatus\", \"type\": \"FLOAT\"}, #38\n",
    "    {\"name\": \"charflag\", \"type\": \"STRING\"}, #39\n",
    "    {\"name\": \"varflag\", \"type\": \"FLOAT\"}, #40\n",
    "    {\"name\": \"batchHeaderID\", \"type\": \"BOOLEAN\"}, #41\n",
    "    {\"name\": \"local\", \"type\": \"FLOAT\"}, #42\n",
    "    {\"name\": \"organic\", \"type\": \"FLOAT\"}, #43\n",
    "    {\"name\": \"display\", \"type\": \"BOOLEAN\"}, #44\n",
    "    {\"name\": \"receipt\", \"type\": \"FLOAT\"}, #45\n",
    "    {\"name\": \"card_no\", \"type\": \"FLOAT\"}, #46\n",
    "    {\"name\": \"store\", \"type\": \"FLOAT\"}, #47\n",
    "    {\"name\": \"branch\", \"type\": \"FLOAT\"}, #48\n",
    "    {\"name\": \"match_id\", \"type\": \"FLOAT\"}, #49\n",
    "    {\"name\": \"trans_id\", \"type\": \"FLOAT\"} #50\n",
    "]\n",
    "\n",
    "date_columns = [\"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where all of the cleaned wedge zip files are kept\n",
    "folder_path = 'wedge_zips'\n",
    "\n",
    "# Create a folder to store the extracted CSV files\n",
    "output_folder = 'wedge_extracted'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.zip'):\n",
    "        # Construct the full path to the zip file\n",
    "        zip_file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Open the zip file\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "            # Extract all files from the zip archive\n",
    "            zip_file.extractall(output_folder)\n",
    "            \n",
    "            # In case more than one csv is in the zip\n",
    "            extracted_files = zip_file.namelist()\n",
    "            if len(extracted_files) == 1 and extracted_files[0].endswith('.csv'):\n",
    "                csv_file_path = os.path.join(output_folder, extracted_files[0])\n",
    "                new_csv_name = os.path.splitext(filename)[0] + '.csv'\n",
    "                os.rename(csv_file_path, os.path.join(output_folder, new_csv_name))\n",
    "\n",
    "print(\"CSV files extracted and saved in the 'wedge_extracted' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where all the extracted csv files live\n",
    "folder_path = 'wedge_extracted'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# The values we want to be recognized as None when pandas reads them in\n",
    "custom_na_values = [r\"\\\\N\", r\"\\N\", \"nan\", None, np.NaN]\n",
    "\n",
    "# The organic column has an empty string but its a float column \n",
    "# So these are values we want replaced with None\n",
    "replacement_dict = {' ': None, '': None, '  ': None}\n",
    "\n",
    "for file in csv_files:\n",
    "    # Use the CSV filename as the table name\n",
    "    table_id = os.path.basename(file).split('.')[0]  \n",
    "\n",
    "    # Construct the BigQuery table reference\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Load the CSV data into the BigQuery table with the specified schema\n",
    "    df = pd.read_csv(file, parse_dates=date_columns, na_values=custom_na_values, keep_default_na=True)\n",
    "\n",
    "    column_types = {\n",
    "    #\"datetime\": \"TIMESTAMP\", #1\n",
    "    \"register_no\": float, #2\n",
    "    \"emp_no\": float, #3\n",
    "    \"trans_no\": float, #4\n",
    "    \"upc\": str, #5\n",
    "    \"description\": str, #6\n",
    "    \"trans_type\": str, #7\n",
    "    \"trans_subtype\": str, #8\n",
    "    \"trans_status\": str, #9\n",
    "    \"department\": float, #10\n",
    "    \"quantity\": float, #11\n",
    "    \"Scale\": float, #12\n",
    "    \"cost\": float, #13\n",
    "    \"unitPrice\": float, #14\n",
    "    \"total\": float, #15\n",
    "    \"regPrice\": float, #16\n",
    "    \"altPrice\": float, #17\n",
    "    \"tax\": float, #18\n",
    "    \"taxexempt\": float, #19\n",
    "    \"foodstamp\": float, #20\n",
    "    \"wicable\": float, #21\n",
    "    \"discount\": float, #22\n",
    "    \"memDiscount\": float, #23\n",
    "    \"discountable\": float, #24\n",
    "    \"discounttype\": float, #25\n",
    "    \"voided\": float, #26\n",
    "    \"percentDiscount\": float, #27\n",
    "    \"ItemQtty\": float, #28\n",
    "    \"volDiscType\": float, #29\n",
    "    \"volume\": float, #30\n",
    "    \"VolSpecial\": float, #31\n",
    "    \"mixMatch\": float, #32\n",
    "    \"matched\": float, #33\n",
    "    \"memType\": bool, #34\n",
    "    \"staff\": bool, #35\n",
    "    \"numflag\": float, #36\n",
    "    \"itemstatus\": float, #37\n",
    "    \"tenderstatus\": float, #38\n",
    "    \"charflag\": str, #39\n",
    "    \"varflag\": float, #40\n",
    "    \"batchHeaderID\": bool, #41\n",
    "    \"local\": float, #42\n",
    "    \"organic\": float, #43\n",
    "    \"display\": bool, #44\n",
    "    \"receipt\": float, #45\n",
    "    \"card_no\": float, #46\n",
    "    \"store\": float, #47\n",
    "    \"branch\": float, #48\n",
    "    \"match_id\": float, #49\n",
    "    \"trans_id\": float, #50\n",
    "    }\n",
    "\n",
    "    # Use the astype() method to change the data types of specific columns\n",
    "    df['organic'].replace(replacement_dict, inplace=True)\n",
    "    df = df.astype(column_types)\n",
    "    df.replace(\"nan\", None, inplace=True)\n",
    "\n",
    "    # Replace values in the 'organic' column using the dictionary\n",
    "    \n",
    "\n",
    "    pandas_gbq.to_gbq(df, destination_table=table_ref, project_id=project_id, if_exists=\"replace\" , table_schema=schema\n",
    "    )\n",
    "\n",
    "    print(f\"File {file} uploaded to BigQuery table {table_id} in dataset {dataset_id} with the specified schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to upload the department lookup table\n",
    "df = pd.read_csv(\"dept_lookup.csv\")\n",
    "\n",
    "# Get it into wedge_data2 on GBQ\n",
    "pandas_gbq.to_gbq(df, destination_table=\"wedge-project-vw.wedge_data2.dept_lookup\", project_id=project_id, if_exists=\"replace\" \n",
    "    )\n",
    "\n",
    "print(f\"File uploaded to BigQuery table  with the specified schema.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
